{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669cf8f6-c172-4724-9657-f367bcefd811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for manipulating the PDF\n",
    "# import fitz\n",
    "# for OCR using PyTesseract\n",
    "import re\n",
    "import os\n",
    "import cv2                              # pre-processing images\n",
    "import math\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytesseract                      # extracting text from images\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt         # displaying output images\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c57757a-2cc7-4410-950f-e0cda4944347",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\alper\\\\OneDrive\\\\Desktop\\\\School\\\\5th Semester\\\\AIN311\\\\Project') \n",
    "french_filepaths = []\n",
    "moai_filepaths = []\n",
    "worldcup_filepaths = []\n",
    "french_filepaths = [os.path.join(\"data\\\\french\\\\\",f) for f in os.listdir(\"data\\\\french\\\\\") if f.endswith(\".txt\")]\n",
    "moai_filepaths = [os.path.join(\"data\\\\moai\\\\\",f) for f in os.listdir(\"data\\\\moai\\\\\") if f.endswith(\".txt\")]\n",
    "worldcup_filepaths = [os.path.join(\"data\\\\worldcup\\\\\",f) for f in os.listdir(\"data\\\\worldcup\\\\\") if f.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6405c6e-2fda-42a7-8d56-8ce06c074fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_experiments = []\n",
    "for data_file in french_filepaths:\n",
    "    df = pd.read_json(data_file, lines = True)\n",
    "    df[\"values\"]\n",
    "    french_experiments.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3bdc853-bf33-4878-8e44-062819ce8bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " french : \n",
      "0:841, 1:749, 2:345, 3:334, 4:322, 5:889, 6:140, 7:415, 8:246, 9:602, 10:865, 11:542, 12:308, 13:325, 14:228, 15:924, 16:660, \n",
      " moai : \n",
      "0:656, 1:527, 2:290, 3:374, 4:422, 5:193, 6:44, 7:521, 8:271, 9:614, 10:625, 11:544, 12:362, 13:230, 14:250, 15:754, 16:499, \n",
      " worldcup : \n",
      "0:673, 1:533, 2:261, 3:336, 4:266, 5:242, 6:91, 7:313, 8:300, 9:475, 10:537, 11:427, 12:208, 13:144, 14:274, 15:726, 16:626, "
     ]
    }
   ],
   "source": [
    "for text_type, filepaths in zip([\"french\",\"moai\",\"worldcup\"], [french_filepaths, moai_filepaths, worldcup_filepaths]):\n",
    "    # Read data\n",
    "    generic_datalist = []\n",
    "    for data_file in filepaths:\n",
    "        with open(data_file) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        dataDict = {}\n",
    "        for i,line in enumerate(lines):\n",
    "            lineDict = json.loads(line)\n",
    "            dataDict[i] = lineDict\n",
    "        generic_datalist.append(dataDict)\n",
    "\n",
    "    # filter out non-fixation data\n",
    "    generic_fixation_list = []\n",
    "    for experiment_dict in generic_datalist:\n",
    "        trackerDict = {}\n",
    "        j = 0\n",
    "        for i in range(len(experiment_dict)):\n",
    "            if(experiment_dict[i]['category'] == 'tracker'):\n",
    "                trackerDict[j] = experiment_dict[i]\n",
    "                j += 1\n",
    "\n",
    "        fixationDict = {}\n",
    "        j = 0\n",
    "        for i in range(len(trackerDict)):\n",
    "            try:\n",
    "                if(trackerDict[i]['values']['frame']['fix'] == True):\n",
    "                    fixationDict[j] = trackerDict[i]\n",
    "                    j += 1\n",
    "            except:\n",
    "                pass\n",
    "        generic_fixation_list.append(fixationDict)\n",
    "\n",
    "    # general overview\n",
    "    print(\"\\n\",text_type,\": \")\n",
    "    for i,experiment in enumerate(generic_fixation_list):\n",
    "        print(str(i) + \":\"+ str(len(experiment)), end = \", \")\n",
    "\n",
    "    # df creation for cond\n",
    "    generic_list = []\n",
    "    for experiment_dict in generic_fixation_list:\n",
    "        experiment_df = pd.DataFrame( columns=list('xy'))\n",
    "        for i in range(len(experiment_dict)):\n",
    "            x = experiment_dict[i]['values']['frame']['raw']['x']\n",
    "            y = experiment_dict[i]['values']['frame']['raw']['y']   \n",
    "            experiment_df = experiment_df.append({'x':x,'y':y}, ignore_index=True) \n",
    "        generic_list.append(experiment_df)\n",
    "\n",
    "    if (text_type == \"french\"):\n",
    "        french_df = generic_list\n",
    "    elif (text_type == \"moai\"):\n",
    "        moai_df = generic_list\n",
    "    elif (text_type == \"worldcup\"):\n",
    "        worldcup_df = generic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73d0719d-7c7f-4697-8095-9bf18056d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_BoW_list = []\n",
    "moai_BoW_list = []\n",
    "worldcup_BoW_list = []\n",
    "for text_type, dataset in zip([\"french\",\"moai\",\"worldcup\"], [french_df, moai_df, worldcup_df]):\n",
    "    screenshot_filepath = \"data\\\\\" + text_type + \"\\\\\" + text_type + \".png\"\n",
    "    original_image = cv2.imread(screenshot_filepath)\n",
    "    # convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "    # Performing OTSU threshold\n",
    "    ret, threshold_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    rectangular_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (12, 12))\n",
    "\n",
    "    # Applying dilation on the threshold image\n",
    "    dilated_image = cv2.dilate(threshold_image, rectangular_kernel, iterations = 1)\n",
    "    #plt.figure(figsize=(25, 15))\n",
    "    #plt.imshow(dilated_image)\n",
    "    #plt.show()\n",
    "\n",
    "    # Finding contours\n",
    "    contours, hierarchy = cv2.findContours(dilated_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Creating a copy of the image\n",
    "    copied_image = original_image.copy()\n",
    "\n",
    "    mask = np.zeros(original_image.shape, np.uint8)\n",
    "\n",
    "    # Looping through the identified contours\n",
    "    # Then rectangular part is cropped and passed on to pytesseract\n",
    "    # pytesseract extracts the text inside each contours\n",
    "    # Extracted text is then written into a text file\n",
    "    paragraph = \"\"\n",
    "    #print(len(contours))\n",
    "    for experiment_data in dataset:\n",
    "        bag_of_words = {}\n",
    "        #print(experiment_data)\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            # Cropping the text block for giving input to OCR\n",
    "            cropped = copied_image[y:y + h, x:x + w]\n",
    "            \n",
    "            cv2.rectangle(copied_image, (x, y), (x + w, y + h), (36,255,12), 2)\n",
    "            # Apply OCR on the cropped image\n",
    "            text = pytesseract.image_to_string(cropped, lang='eng', config='--oem 3 --psm 1')\n",
    "            text = text.lower()\n",
    "            text = re.sub('[^a-z]', ' ', text)\n",
    "            text = re.sub(r'\\s+', '', text)\n",
    "\n",
    "            insideCond = (experiment_data[\"x\"] >= x) & (experiment_data[\"x\"] < x + w) & (experiment_data[\"y\"] >= y) & (experiment_data[\"y\"] < y + h)\n",
    "            #print(insideCond)\n",
    "            boundFixations = experiment_data[insideCond]\n",
    "            #print(boundFixations)\n",
    "            for i in range(len(boundFixations)):\n",
    "                try:\n",
    "                    count = bag_of_words[str(text)]\n",
    "                    count += 1\n",
    "                    bag_of_words.update({str(text):count})\n",
    "                except:\n",
    "                    bag_of_words[str(text)] = 1\n",
    "            #del bag_of_words[\"\"]       \n",
    "        if (text_type == \"french\"):\n",
    "            french_BoW_list.append(bag_of_words)\n",
    "            #print(french_BoW_list)\n",
    "        elif (text_type == \"moai\"):\n",
    "            moai_BoW_list.append(bag_of_words)\n",
    "            #print(moai_BoW_list)\n",
    "        elif (text_type == \"worldcup\"):\n",
    "            worldcup_BoW_list.append(bag_of_words)\n",
    "            #print(worldcup_BoW_list)\n",
    "    masked = cv2.drawContours(mask, [cnt], 0, (255, 255, 255), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "73d2b86b-54f3-42b4-9be4-0758ca62e9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assembly': 4,\n",
       " 'national': 1,\n",
       " 'convocation': 9,\n",
       " 'ofthe': 3,\n",
       " 'the': 14,\n",
       " 'distress': 12,\n",
       " 'manage': 27,\n",
       " 'may': 2,\n",
       " 'ancienrgime': 8,\n",
       " 'widespread': 10,\n",
       " 'unableto': 7,\n",
       " 'proved': 5,\n",
       " 'economic': 12,\n",
       " 'factors': 20,\n",
       " 'political': 16,\n",
       " 'social': 10,\n",
       " 'combination': 2,\n",
       " 'are': 1,\n",
       " 'be': 6,\n",
       " 'agreed': 6,\n",
       " 'generally': 25,\n",
       " 'day': 6,\n",
       " 'this': 5,\n",
       " 'politics': 8,\n",
       " 'institutions': 4,\n",
       " 'dominate': 13,\n",
       " 'created': 3,\n",
       " 'values': 2,\n",
       " 'suffrage': 6,\n",
       " 'campaigns': 13,\n",
       " 'universal': 22,\n",
       " 'slavery': 7,\n",
       " 'abolition': 25,\n",
       " 'for': 7,\n",
       " 'inspired': 5,\n",
       " 'russian': 1,\n",
       " 'such': 3,\n",
       " 'revolts': 1}"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_BoW_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2e4467fb-f87c-484d-850a-d53225e4a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_y = [1,5,5,3,4,4,1,3,4,1,4,2,3,3,5,2,2]\n",
    "moai_y = [1,1,1,4,2,1,2,1,1,1,2,3,1,1,3,1,1]\n",
    "worldcup_y = [2,5,1,1,2,1,3,4,4,3,1,5,4,3,3,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e41df4e4-c41f-41c0-9664-053929650de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_BoW_list = french_BoW_list + moai_BoW_list + worldcup_BoW_list\n",
    "all_y =  french_y + moai_y + worldcup_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456076cf-0199-4ad9-85bb-04f362422d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bag_of_words in all_BoW_list:\n",
    "    del bag_of_words['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "eccefcb5-3527-46e1-b14e-583d37ef61cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_BoW_list, all_y, test_size=0.15, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c43c5377-4f90-4a0b-a400-50cb9255db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertBoWtoVector(bag_of_words):\n",
    "    vector = []\n",
    "    for count in bag_of_words.values():\n",
    "        vector.append(count)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "195d2855-0fd8-477c-b084-3920baf1ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnNc(category, X_train):\n",
    "    total = 0\n",
    "    ls = []\n",
    "    for i in range(len(X_train)):\n",
    "        ls.append([y_train[i],X_train[i]])\n",
    "        \n",
    "    for i in range(len(ls)):\n",
    "        if (ls[i][0] == int(category)):\n",
    "            total += 1\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c59b2cfb-e054-465b-89f1-0f972861feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBagofBags(X_train,y_train):\n",
    "    ls = []\n",
    "    for i in range(len(X_train)):\n",
    "        ls.append([y_train[i],X_train[i]])\n",
    "\n",
    "    bagOfbags = dict([('1',{}),('2',{}),('3',{}),('4',{}),('5',{})])\n",
    "    for i in range(len(ls)):\n",
    "        category = str(ls[i][0])\n",
    "        for word, increase in ls[i][1].items():\n",
    "            try:\n",
    "                count = bag_of_words[str(word)]\n",
    "                count += increase\n",
    "                bagOfbags[category].update({str(word):count})\n",
    "            except:\n",
    "                bagOfbags[category].update({str(word):increase})\n",
    "    return bagOfbags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "29620506-7405-497b-a5c7-21f38fcbd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalUniqueWords(X_train):\n",
    "    word_list = []\n",
    "    for bag_of_words in X_train:\n",
    "        word_list += list(bag_of_words.keys())\n",
    "    vocab_set = set(word_list)\n",
    "    return len(vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b932e271-c042-49b7-ae25-2328b224b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnSizeofCategory(category, bagofBags):\n",
    "    total = 0\n",
    "    for count in bagofBags[category].values():\n",
    "        total += count\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d2e84d05-61aa-49c3-bee1-0912fdb21851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnSizeofAll():\n",
    "    total = 0\n",
    "    for category in ['1','2','3','4','5']:\n",
    "        total += returnSizeofCategory(category)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "483ea2fc-ff0e-4792-a0f7-5f3fe701c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayes (bagOfexperiment, X_train, y_train):\n",
    "    bagofBags = createBagofBags(X_train,y_train)\n",
    "    total_unique_words = getTotalUniqueWords(X_train)\n",
    "    outputDict = dict.fromkeys(bagOfbags.keys())\n",
    "    N = len(X_train)\n",
    "    for category in bagOfbags.keys():\n",
    "        genreBag = bagOfbags[category]\n",
    "        total_size_of_class = returnSizeofCategory(category, bagofBags) # count(c)\n",
    "        N_c = returnNc(category, X_train)\n",
    "        prior = np.log(float(N_c/N))\n",
    "        for word in bagOfexperiment:\n",
    "            try:\n",
    "                word_count_in_class = genreBag[word] # count(w,c)\n",
    "            except:\n",
    "                word_count_in_class = 0\n",
    "            for i in range(int(bagOfexperiment[word])):\n",
    "                P_w_c = float(word_count_in_class + 1)/float(total_size_of_class + total_unique_words)\n",
    "                prior += np.log(P_w_c)\n",
    "        #print('Value for ',genre,': ',prior)\n",
    "        outputDict[category] = prior\n",
    "    max_key = max(outputDict, key=outputDict.get)\n",
    "    return max_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "664f7c29-e516-479f-bdc6-2c08d9791c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy (X_train,y_train,X_test, y_test):\n",
    "    NaiveBayesResults = []\n",
    "    for bag_of_words in X_test:\n",
    "            NaiveBayesResults.append(NaiveBayes(bag_of_words,X_train,y_train))\n",
    "    N = len(X_test)\n",
    "    score = 0\n",
    "    for i in range (len(X_test)):\n",
    "        if(int(NaiveBayesResults[i]) == y_test[i]):\n",
    "            score += 1\n",
    "    return (score/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "8b741482-633d-43a3-bc32-7fbfb35de3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loo_X (all_BoW_list, train_index):\n",
    "    return_list = []\n",
    "    for i in train_index:\n",
    "        return_list.append(all_BoW_list[i])\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "55b2a6a3-700e-4056-a8ef-d8a72dfb2b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[307], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (train_index, test_index) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loo\u001b[38;5;241m.\u001b[39msplit(all_BoW_list)):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m( \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloo_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_BoW_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloo_X\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mloo_X\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_BoW_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloo_X\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m )\n",
      "Cell \u001b[1;32mIn[305], line 4\u001b[0m, in \u001b[0;36maccuracy\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m      2\u001b[0m NaiveBayesResults \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bag_of_words \u001b[38;5;129;01min\u001b[39;00m X_test:\n\u001b[1;32m----> 4\u001b[0m         NaiveBayesResults\u001b[38;5;241m.\u001b[39mappend(\u001b[43mNaiveBayes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbag_of_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_test)\n\u001b[0;32m      6\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[256], line 9\u001b[0m, in \u001b[0;36mNaiveBayes\u001b[1;34m(bagOfexperiment, X_train, y_train)\u001b[0m\n\u001b[0;32m      7\u001b[0m genreBag \u001b[38;5;241m=\u001b[39m bagOfbags[category]\n\u001b[0;32m      8\u001b[0m total_size_of_class \u001b[38;5;241m=\u001b[39m returnSizeofCategory(category, bagofBags) \u001b[38;5;66;03m# count(c)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m N_c \u001b[38;5;241m=\u001b[39m \u001b[43mreturnNc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m prior \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mfloat\u001b[39m(N_c\u001b[38;5;241m/\u001b[39mN))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m bagOfexperiment:\n",
      "Cell \u001b[1;32mIn[268], line 5\u001b[0m, in \u001b[0;36mreturnNc\u001b[1;34m(category, X_train)\u001b[0m\n\u001b[0;32m      3\u001b[0m ls \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train)):\n\u001b[1;32m----> 5\u001b[0m     ls\u001b[38;5;241m.\u001b[39mappend([\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m,X_train[i]])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ls)):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (ls[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mint\u001b[39m(category)):\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(all_BoW_list)\n",
    "for i, (train_index, test_index) in enumerate(loo.split(all_BoW_list)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print( accuracy(loo_X(all_BoW_list, train_index)\n",
    "                    , loo_X (all_y, train_index), \n",
    "                    loo_X (all_BoW_list, test_index), loo_X (all_y, test_index)) )\n",
    "    #print(f\"  Train: index={train_index}\")\n",
    "    #print(f\"  Test:  index={test_index}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ain311",
   "language": "python",
   "name": "ain311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
