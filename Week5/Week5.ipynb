{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2bcffd1-3b4e-4569-be63-25025a49ef22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for manipulating the PDF\n",
    "# import fitz\n",
    "# for OCR using PyTesseract\n",
    "import re\n",
    "import os\n",
    "import cv2                              # pre-processing images\n",
    "import math\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytesseract                      # extracting text from images\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt         # displaying output images\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import itertools\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad62348c-ff79-4e53-8a8c-3f1e2a9b07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a3b5bc7-eb0d-44a4-a43b-13e7e1ae40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\alper\\\\OneDrive\\\\Desktop\\\\School\\\\5th Semester\\\\AIN311\\\\Project') \n",
    "french_filepaths = []\n",
    "moai_filepaths = []\n",
    "worldcup_filepaths = []\n",
    "french_filepaths = [os.path.join(\"data\\\\french\\\\\",f) for f in os.listdir(\"data\\\\french\\\\\") if f.endswith(\".txt\")]\n",
    "moai_filepaths = [os.path.join(\"data\\\\moai\\\\\",f) for f in os.listdir(\"data\\\\moai\\\\\") if f.endswith(\".txt\")]\n",
    "worldcup_filepaths = [os.path.join(\"data\\\\worldcup\\\\\",f) for f in os.listdir(\"data\\\\worldcup\\\\\") if f.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06ac879f-6791-475a-9e51-1fa36ebaa9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_experiments = []\n",
    "for data_file in french_filepaths:\n",
    "    df = pd.read_json(data_file, lines = True)\n",
    "    df[\"values\"]\n",
    "    french_experiments.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd22390c-6a91-47ee-bee1-2374f6ff009b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " french : \n",
      "0:841, 1:749, 2:345, 3:334, 4:322, 5:889, 6:140, 7:415, 8:246, 9:602, 10:865, 11:542, 12:308, 13:325, 14:228, 15:924, 16:660, \n",
      " moai : \n",
      "0:656, 1:527, 2:290, 3:374, 4:422, 5:193, 6:44, 7:521, 8:271, 9:614, 10:625, 11:544, 12:362, 13:230, 14:250, 15:754, 16:499, \n",
      " worldcup : \n",
      "0:673, 1:533, 2:261, 3:336, 4:266, 5:242, 6:91, 7:313, 8:300, 9:475, 10:537, 11:427, 12:208, 13:144, 14:274, 15:726, 16:626, "
     ]
    }
   ],
   "source": [
    "for text_type, filepaths in zip([\"french\",\"moai\",\"worldcup\"], [french_filepaths, moai_filepaths, worldcup_filepaths]):\n",
    "    # Read data\n",
    "    generic_datalist = []\n",
    "    for data_file in filepaths:\n",
    "        with open(data_file) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        dataDict = {}\n",
    "        for i,line in enumerate(lines):\n",
    "            lineDict = json.loads(line)\n",
    "            dataDict[i] = lineDict\n",
    "        generic_datalist.append(dataDict)\n",
    "\n",
    "    # filter out non-fixation data\n",
    "    generic_fixation_list = []\n",
    "    for experiment_dict in generic_datalist:\n",
    "        trackerDict = {}\n",
    "        j = 0\n",
    "        for i in range(len(experiment_dict)):\n",
    "            if(experiment_dict[i]['category'] == 'tracker'):\n",
    "                trackerDict[j] = experiment_dict[i]\n",
    "                j += 1\n",
    "\n",
    "        fixationDict = {}\n",
    "        j = 0\n",
    "        for i in range(len(trackerDict)):\n",
    "            try:\n",
    "                if(trackerDict[i]['values']['frame']['fix'] == True):\n",
    "                    fixationDict[j] = trackerDict[i]\n",
    "                    j += 1\n",
    "            except:\n",
    "                pass\n",
    "        generic_fixation_list.append(fixationDict)\n",
    "\n",
    "    # general overview\n",
    "    print(\"\\n\",text_type,\": \")\n",
    "    for i,experiment in enumerate(generic_fixation_list):\n",
    "        print(str(i) + \":\"+ str(len(experiment)), end = \", \")\n",
    "\n",
    "    # df creation for cond\n",
    "    generic_list = []\n",
    "    for experiment_dict in generic_fixation_list:\n",
    "        experiment_df = pd.DataFrame( columns=list('xy'))\n",
    "        for i in range(len(experiment_dict)):\n",
    "            x = experiment_dict[i]['values']['frame']['raw']['x']\n",
    "            y = experiment_dict[i]['values']['frame']['raw']['y']   \n",
    "            experiment_df = experiment_df.append({'x':x,'y':y}, ignore_index=True) \n",
    "        generic_list.append(experiment_df)\n",
    "\n",
    "    if (text_type == \"french\"):\n",
    "        french_df = generic_list\n",
    "    elif (text_type == \"moai\"):\n",
    "        moai_df = generic_list\n",
    "    elif (text_type == \"worldcup\"):\n",
    "        worldcup_df = generic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791265cf-8b3c-495b-bc48-672caea5d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_BoW_list = []\n",
    "moai_BoW_list = []\n",
    "worldcup_BoW_list = []\n",
    "for text_type, dataset in zip([\"french\",\"moai\",\"worldcup\"], [french_df, moai_df, worldcup_df]):\n",
    "    screenshot_filepath = \"raw_data\\\\\" + text_type + \"\\\\\" + text_type + \".png\"\n",
    "    original_image = cv2.imread(screenshot_filepath)\n",
    "    # convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "    # Performing OTSU threshold\n",
    "    ret, threshold_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    rectangular_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (12, 12))\n",
    "\n",
    "    # Applying dilation on the threshold image\n",
    "    dilated_image = cv2.dilate(threshold_image, rectangular_kernel, iterations = 1)\n",
    "    #plt.figure(figsize=(25, 15))\n",
    "    #plt.imshow(dilated_image)\n",
    "    #plt.show()\n",
    "\n",
    "    # Finding contours\n",
    "    contours, hierarchy = cv2.findContours(dilated_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Creating a copy of the image\n",
    "    copied_image = original_image.copy()\n",
    "\n",
    "    mask = np.zeros(original_image.shape, np.uint8)\n",
    "\n",
    "    # Looping through the identified contours\n",
    "    # Then rectangular part is cropped and passed on to pytesseract\n",
    "    # pytesseract extracts the text inside each contours\n",
    "    # Extracted text is then written into a text file\n",
    "    paragraph = \"\"\n",
    "    #print(len(contours))\n",
    "    for experiment_data in dataset:\n",
    "        bag_of_words = {}\n",
    "        #print(experiment_data)\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            # Cropping the text block for giving input to OCR\n",
    "            cropped = copied_image[y:y + h, x:x + w]\n",
    "            \n",
    "            cv2.rectangle(copied_image, (x, y), (x + w, y + h), (36,255,12), 2)\n",
    "            # Apply OCR on the cropped image\n",
    "            text = pytesseract.image_to_string(cropped, lang='eng', config='--oem 3 --psm 1')\n",
    "            text = text.lower()\n",
    "            text = re.sub('[^a-z]', ' ', text)\n",
    "            text = re.sub(r'\\s+', '', text)\n",
    "\n",
    "            insideCond = (experiment_data[\"x\"] >= x) & (experiment_data[\"x\"] < x + w) & (experiment_data[\"y\"] >= y) & (experiment_data[\"y\"] < y + h)\n",
    "            #print(insideCond)\n",
    "            boundFixations = experiment_data[insideCond]\n",
    "            #print(boundFixations)\n",
    "            for i in range(len(boundFixations)):\n",
    "                try:\n",
    "                    count = bag_of_words[str(text)]\n",
    "                    count += 1\n",
    "                    bag_of_words.update({str(text):count})\n",
    "                except:\n",
    "                    bag_of_words[str(text)] = 1\n",
    "            #del bag_of_words[\"\"]       \n",
    "        if (text_type == \"french\"):\n",
    "            french_BoW_list.append(bag_of_words)\n",
    "            #print(french_BoW_list)\n",
    "        elif (text_type == \"moai\"):\n",
    "            moai_BoW_list.append(bag_of_words)\n",
    "            #print(moai_BoW_list)\n",
    "        elif (text_type == \"worldcup\"):\n",
    "            worldcup_BoW_list.append(bag_of_words)\n",
    "            #print(worldcup_BoW_list)\n",
    "    masked = cv2.drawContours(mask, [cnt], 0, (255, 255, 255), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7991ff7b-3464-4dd1-a099-a8dafa30cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_list = []\n",
    "#We need a fixed size a vector so we summarized each bag of words and make them a set of vocabulary\n",
    "for bag_of_words in french_BoW_list: #Bag of words of each subject\n",
    "    french_list += list(bag_of_words.keys()) #gather all words across different experiments' bag_of_words\n",
    "french_vocab_set = dict.fromkeys(set(french_list), 0) #initialize an empty dictionary with all counts as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0d46516-65d4-40bd-b859-6a7e9312bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bag_of_words in french_BoW_list:\n",
    "    for word in bag_of_words.keys():\n",
    "        count = french_vocab_set[str(word)]\n",
    "        count += 1\n",
    "        french_vocab_set.update({str(word):count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aa56c2b-c883-421c-ade1-dd96bb0262eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertBoWtoVector(bag_of_words):\n",
    "    vector = []\n",
    "    for count in bag_of_words.values():\n",
    "        vector.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8473593d-82c6-4c97-8ae0-46f51b93596d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function creates a fixed size vector of the universal set (U) \n",
    "# of vocabulary used across  all bag of words\n",
    "def convertToAllDict (BoW_list, bag_of_words):\n",
    "    french_list = []\n",
    "    for bag_of_words1 in BoW_list:\n",
    "        french_list += list(bag_of_words1.keys())\n",
    "    french_vocab_set = dict.fromkeys(set(french_list), 0)\n",
    "    \n",
    "    for word in bag_of_words.keys():\n",
    "        french_vocab_set.update({str(word):bag_of_words[str(word)]})\n",
    "    return convertBoWtoVector(french_vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "a9d6a837-fe1f-46a3-8d4f-3fea7351fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New td-idf lists for all 3 classes are created using convertToAllDict() function above\n",
    "train_frenchNew = [] # French td-idf list\n",
    "idx = 0\n",
    "for bag_of_word in french_BoW_list:\n",
    "    bag_of_word_tfidf = calculate_tf_idf(bag_of_word)\n",
    "    x = convertToAllDict(french_BoW_list, bag_of_word_tfidf) \n",
    "    x = sum(x)\n",
    "    train_frenchNew.append([x,train_french[idx][1]] )\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "9b188f79-22df-4127-94e7-262c893a49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_worldcupNew = [] # Worldcup td-idf list\n",
    "idx=0\n",
    "for bag_of_word in worldcup_BoW_list:\n",
    "    bag_of_word_tfidf=calculate_tf_idf(bag_of_word)\n",
    "    x=convertToAllDict(worldcup_BoW_list,bag_of_word_tfidf)\n",
    "    x=sum(x)\n",
    "    train_worldcupNew.append( [x,train_worldcup[idx][1]] )\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "6a7db0b5-618b-4a5f-9be5-949bfafa20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_moaiNew = [] # Moai td-idf list\n",
    "idx=0\n",
    "for bag_of_word in moai_BoW_list:\n",
    "    bag_of_word_tfidf=calculate_tf_idf(bag_of_word)\n",
    "    x=convertToAllDict(moai_BoW_list,bag_of_word_tfidf)\n",
    "    x=sum(x)\n",
    "    train_moaiNew.append( [x,train_moai[idx][1]] )\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "d9d8cf59-7e1a-4932-8a5b-5703d9ff2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 17 folds for each of 100 candidates, totalling 1700 fits\n",
      "Random grid:  {'n_estimators': [5, 20, 50, 100], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120], 'min_samples_split': [2, 6, 10], 'min_samples_leaf': [1, 3, 4], 'bootstrap': [True, False]} \n",
      "\n",
      "Best Parameters:  {'n_estimators': 5, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': 50, 'bootstrap': True}  \n",
      "\n",
      "Accuracy: 0.118 (0.322)\n",
      "Fitting 17 folds for each of 100 candidates, totalling 1700 fits\n",
      "Random grid:  {'n_estimators': [5, 20, 50, 100], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120], 'min_samples_split': [2, 6, 10], 'min_samples_leaf': [1, 3, 4], 'bootstrap': [True, False]} \n",
      "\n",
      "Best Parameters:  {'n_estimators': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 70, 'bootstrap': True}  \n",
      "\n",
      "Accuracy: 0.000 (0.000)\n",
      "Fitting 17 folds for each of 100 candidates, totalling 1700 fits\n",
      "Random grid:  {'n_estimators': [5, 20, 50, 100], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120], 'min_samples_split': [2, 6, 10], 'min_samples_leaf': [1, 3, 4], 'bootstrap': [True, False]} \n",
      "\n",
      "Best Parameters:  {'n_estimators': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 110, 'bootstrap': True}  \n",
      "\n",
      "Accuracy: 0.000 (0.000)\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    if idx==0:\n",
    "        print(\"French Dataset Results:\")\n",
    "    if idx==1:\n",
    "        print(\"Moai Dataset Results:\")\n",
    "    if idx==2:\n",
    "        print(\"World Cup Dataset Results:\")\n",
    "    n_estimators = [5,20,50,100] # number of trees in the random forest\n",
    "    max_features = ['auto', 'sqrt'] # number of features in consideration at every split\n",
    "    max_depth = [int(x) for x in np.linspace(10, 120, num = 12)] # maximum number of levels allowed in each decision tree\n",
    "    min_samples_split = [2, 6, 10] # minimum sample number to split a node\n",
    "    min_samples_leaf = [1, 3, 4] # minimum sample number that can be stored in a leaf node\n",
    "    bootstrap = [True, False] # method used to sample data points\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap}\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    rf_random = RandomizedSearchCV(estimator = clf,param_distributions = random_grid,\n",
    "                   n_iter = 100, cv = cv, verbose=2, random_state=35, n_jobs = -1)\n",
    "    X=dataset.loc[:, dataset.columns == 'X']\n",
    "    y=dataset.loc[:, dataset.columns != 'X']\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    X_transformed = label_encoder.fit_transform(X)\n",
    "    X=X_transformed.reshape(-1,1)\n",
    "    rf_random.fit(X, y)\n",
    "    print ('Random grid: ', random_grid, '\\n')\n",
    "    # print the best parameters\n",
    "    print ('Best Parameters: ', rf_random.best_params_, ' \\n')\n",
    "   \n",
    "    randmf=RandomForestClassifier(n_estimators= 50, min_samples_split= 10, min_samples_leaf= 3, max_features= \"sqrt\", max_depth= 80, bootstrap= True)\n",
    "    scores_randmf = cross_val_score(randmf, X, y, scoring='accuracy',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(scores_randmf), std(scores_randmf)))\n",
    "    idx+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ain311",
   "language": "python",
   "name": "ain311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
