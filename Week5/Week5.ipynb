{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2bcffd1-3b4e-4569-be63-25025a49ef22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for manipulating the PDF\n",
    "# import fitz\n",
    "# for OCR using PyTesseract\n",
    "import re\n",
    "import os\n",
    "import cv2                              # pre-processing images\n",
    "import math\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytesseract                      # extracting text from images\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt         # displaying output images\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import itertools\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad62348c-ff79-4e53-8a8c-3f1e2a9b07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3b5bc7-eb0d-44a4-a43b-13e7e1ae40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_filepaths = []\n",
    "moai_filepaths = []\n",
    "worldcup_filepaths = []\n",
    "french_filepaths = [os.path.join(\"raw_data\\\\french\\\\\",f) for f in os.listdir(\"raw_data\\\\french\\\\\") if f.endswith(\".txt\")]\n",
    "moai_filepaths = [os.path.join(\"raw_data\\\\moai\\\\\",f) for f in os.listdir(\"raw_data\\\\moai\\\\\") if f.endswith(\".txt\")]\n",
    "worldcup_filepaths = [os.path.join(\"raw_data\\\\worldcup\\\\\",f) for f in os.listdir(\"raw_data\\\\worldcup\\\\\") if f.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ac879f-6791-475a-9e51-1fa36ebaa9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_experiments = []\n",
    "for data_file in french_filepaths:\n",
    "    df = pd.read_json(data_file, lines = True)\n",
    "    df[\"values\"]\n",
    "    french_experiments.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd22390c-6a91-47ee-bee1-2374f6ff009b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " french : \n",
      "0:841, 1:749, 2:345, 3:334, 4:322, 5:889, 6:140, 7:415, 8:246, 9:602, 10:865, 11:542, 12:308, 13:325, 14:228, 15:924, 16:660, \n",
      " moai : \n",
      "0:656, 1:527, 2:290, 3:374, 4:422, 5:193, 6:44, 7:521, 8:271, 9:614, 10:625, 11:544, 12:362, 13:230, 14:250, 15:754, 16:499, \n",
      " worldcup : \n",
      "0:673, 1:533, 2:261, 3:336, 4:266, 5:242, 6:91, 7:313, 8:300, 9:475, 10:537, 11:427, 12:208, 13:144, 14:274, 15:726, 16:626, "
     ]
    }
   ],
   "source": [
    "for text_type, filepaths in zip([\"french\",\"moai\",\"worldcup\"], [french_filepaths, moai_filepaths, worldcup_filepaths]):\n",
    "    # Read data\n",
    "    generic_datalist = []\n",
    "    for data_file in filepaths:\n",
    "        with open(data_file) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        dataDict = {}\n",
    "        for i,line in enumerate(lines):\n",
    "            lineDict = json.loads(line)\n",
    "            dataDict[i] = lineDict\n",
    "        generic_datalist.append(dataDict)\n",
    "\n",
    "    # filter out non-fixation data\n",
    "    generic_fixation_list = []\n",
    "    for experiment_dict in generic_datalist:\n",
    "        trackerDict = {}\n",
    "        j = 0\n",
    "        for i in range(len(experiment_dict)):\n",
    "            if(experiment_dict[i]['category'] == 'tracker'):\n",
    "                trackerDict[j] = experiment_dict[i]\n",
    "                j += 1\n",
    "\n",
    "        fixationDict = {}\n",
    "        j = 0\n",
    "        for i in range(len(trackerDict)):\n",
    "            try:\n",
    "                if(trackerDict[i]['values']['frame']['fix'] == True):\n",
    "                    fixationDict[j] = trackerDict[i]\n",
    "                    j += 1\n",
    "            except:\n",
    "                pass\n",
    "        generic_fixation_list.append(fixationDict)\n",
    "\n",
    "    # general overview\n",
    "    print(\"\\n\",text_type,\": \")\n",
    "    for i,experiment in enumerate(generic_fixation_list):\n",
    "        print(str(i) + \":\"+ str(len(experiment)), end = \", \")\n",
    "\n",
    "    # df creation for cond\n",
    "    generic_list = []\n",
    "    for experiment_dict in generic_fixation_list:\n",
    "        experiment_df = pd.DataFrame( columns=list('xy'))\n",
    "        for i in range(len(experiment_dict)):\n",
    "            x = experiment_dict[i]['values']['frame']['raw']['x']\n",
    "            y = experiment_dict[i]['values']['frame']['raw']['y']   \n",
    "            experiment_df = experiment_df.append({'x':x,'y':y}, ignore_index=True) \n",
    "        generic_list.append(experiment_df)\n",
    "\n",
    "    if (text_type == \"french\"):\n",
    "        french_df = generic_list\n",
    "    elif (text_type == \"moai\"):\n",
    "        moai_df = generic_list\n",
    "    elif (text_type == \"worldcup\"):\n",
    "        worldcup_df = generic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb5c3b1-70d6-4a7d-ab56-39cdb20532de",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_BoW_list = []\n",
    "moai_BoW_list = []\n",
    "worldcup_BoW_list = []\n",
    "for text_type, dataset in zip([\"french\",\"moai\",\"worldcup\"], [french_df, moai_df, worldcup_df]):\n",
    "    screenshot_filepath = \"raw_data\\\\\" + text_type + \"\\\\\" + text_type + \".png\"\n",
    "    original_image = cv2.imread(screenshot_filepath)\n",
    "    # convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "    # Performing OTSU threshold\n",
    "    ret, threshold_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    rectangular_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (12, 12))\n",
    "\n",
    "    # Applying dilation on the threshold image\n",
    "    dilated_image = cv2.dilate(threshold_image, rectangular_kernel, iterations = 1)\n",
    "    #plt.figure(figsize=(25, 15))\n",
    "    #plt.imshow(dilated_image)\n",
    "    #plt.show()\n",
    "\n",
    "    # Finding contours\n",
    "    contours, hierarchy = cv2.findContours(dilated_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Creating a copy of the image\n",
    "    copied_image = original_image.copy()\n",
    "\n",
    "    mask = np.zeros(original_image.shape, np.uint8)\n",
    "\n",
    "    # Looping through the identified contours\n",
    "    # Then rectangular part is cropped and passed on to pytesseract\n",
    "    # pytesseract extracts the text inside each contours\n",
    "    # Extracted text is then written into a text file\n",
    "    paragraph = \"\"\n",
    "    #print(len(contours))\n",
    "    for experiment_data in dataset:\n",
    "        bag_of_words = {}\n",
    "        #print(experiment_data)\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            # Cropping the text block for giving input to OCR\n",
    "            cropped = copied_image[y:y + h, x:x + w]\n",
    "            \n",
    "            cv2.rectangle(copied_image, (x, y), (x + w, y + h), (36,255,12), 2)\n",
    "            # Apply OCR on the cropped image\n",
    "            text = pytesseract.image_to_string(cropped, lang='eng', config='--oem 3 --psm 1')\n",
    "            text = text.lower()\n",
    "            text = re.sub('[^a-z]', ' ', text)\n",
    "            text = re.sub(r'\\s+', '', text)\n",
    "\n",
    "            insideCond = (experiment_data[\"x\"] >= x) & (experiment_data[\"x\"] < x + w) & (experiment_data[\"y\"] >= y) & (experiment_data[\"y\"] < y + h)\n",
    "            #print(insideCond)\n",
    "            boundFixations = experiment_data[insideCond]\n",
    "            #print(boundFixations)\n",
    "            for i in range(len(boundFixations)):\n",
    "                try:\n",
    "                    count = bag_of_words[str(text)]\n",
    "                    count += 1\n",
    "                    bag_of_words.update({str(text):count})\n",
    "                except:\n",
    "                    bag_of_words[str(text)] = 1\n",
    "            #del bag_of_words[\"\"]       \n",
    "        if (text_type == \"french\"):\n",
    "            french_BoW_list.append(bag_of_words)\n",
    "            #print(french_BoW_list)\n",
    "        elif (text_type == \"moai\"):\n",
    "            moai_BoW_list.append(bag_of_words)\n",
    "            #print(moai_BoW_list)\n",
    "        elif (text_type == \"worldcup\"):\n",
    "            worldcup_BoW_list.append(bag_of_words)\n",
    "            #print(worldcup_BoW_list)\n",
    "    masked = cv2.drawContours(mask, [cnt], 0, (255, 255, 255), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7991ff7b-3464-4dd1-a099-a8dafa30cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_list = []\n",
    "#We need a fixed size a vector so we summarized each bag of words and make them a set of vocabulary\n",
    "for bag_of_words in french_BoW_list: #Bag of words of each subject\n",
    "    french_list += list(bag_of_words.keys()) #gather all words across different experiments' bag_of_words\n",
    "french_vocab_set = dict.fromkeys(set(french_list), 0) #initialize an empty dictionary with all counts as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "f0d46516-65d4-40bd-b859-6a7e9312bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bag_of_words in french_BoW_list:\n",
    "    for word in bag_of_words.keys():\n",
    "        count = french_vocab_set[str(word)]\n",
    "        count += 1\n",
    "        french_vocab_set.update({str(word):count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa56c2b-c883-421c-ade1-dd96bb0262eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertBoWtoVector(bag_of_words):\n",
    "    vector = []\n",
    "    for count in bag_of_words.values():\n",
    "        vector.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "8473593d-82c6-4c97-8ae0-46f51b93596d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function creates a fixed size vector of the universal set (U) \n",
    "# of vocabulary used across  all bag of words\n",
    "def convertToAllDict (BoW_list, bag_of_words):\n",
    "    french_list = []\n",
    "    for bag_of_words1 in BoW_list:\n",
    "        french_list += list(bag_of_words1.keys())\n",
    "    french_vocab_set = dict.fromkeys(set(french_list), 0)\n",
    "    \n",
    "    for word in bag_of_words.keys():\n",
    "        french_vocab_set.update({str(word):bag_of_words[str(word)]})\n",
    "    return convertBoWtoVector(french_vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "e8db2da8-e39d-4e8d-aee8-ad666b14f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New td-idf lists for all 3 classes are created using convertToAllDict() function above\n",
    "train_frenchNew = [] # French td-idf list\n",
    "idx = 0\n",
    "for bag_of_word in french_BoW_list:\n",
    "    bag_of_word_tfidf = calculate_tf_idf(bag_of_word)\n",
    "    x = convertToAllDict(french_BoW_list, bag_of_word_tfidf) \n",
    "    x = sum(x)\n",
    "    train_frenchNew.append([x,train_french[idx][1]] )\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "67f3a35a-b533-47b1-ba08-124e2e142543",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_worldcupNew = [] # Worldcup td-idf list\n",
    "idx=0\n",
    "for bag_of_word in worldcup_BoW_list:\n",
    "    bag_of_word_tfidf=calculate_tf_idf(bag_of_word)\n",
    "    x=convertToAllDict(worldcup_BoW_list,bag_of_word_tfidf)\n",
    "    x=sum(x)\n",
    "    train_worldcupNew.append( [x,train_worldcup[idx][1]] )\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "2135d2ad-23dd-4699-abed-76039eb023e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_moaiNew = [] # Moai td-idf list\n",
    "idx=0\n",
    "for bag_of_word in moai_BoW_list:\n",
    "    bag_of_word_tfidf=calculate_tf_idf(bag_of_word)\n",
    "    x=convertToAllDict(moai_BoW_list,bag_of_word_tfidf)\n",
    "    x=sum(x)\n",
    "    train_moaiNew.append( [x,train_moai[idx][1]] )\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "b834c88b-9727-4b67-937c-27a2ebd0e0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Dataset Results:\n",
      "Logistic Regression Accuracy: 0.118 (0.322)\n",
      "Logistic Regression f1 Score: 0.118 (0.322)\n",
      "Logistic Regression AUC: nan (nan)\n",
      "Support Vector Machine Accuracy: 0.176 (0.381)\n",
      "Support Vector Machine f1 Score: 0.176 (0.381)\n",
      "Support Vector Machine AUC: nan (nan)\n",
      "Random Forest Accuracy: 0.118 (0.322)\n",
      "Random Forest f1 Score: 0.118 (0.322)\n",
      "Random Forest AUC: nan (nan)\n",
      "\n",
      "\n",
      "Moai Dataset Results:\n",
      "Logistic Regression Accuracy: 0.059 (0.235)\n",
      "Logistic Regression f1 Score: 0.059 (0.235)\n",
      "Logistic Regression AUC: nan (nan)\n",
      "Support Vector Machine Accuracy: 0.118 (0.322)\n",
      "Support Vector Machine f1 Score: 0.118 (0.322)\n",
      "Support Vector Machine AUC: nan (nan)\n",
      "Random Forest Accuracy: 0.294 (0.456)\n",
      "Random Forest f1 Score: 0.294 (0.456)\n",
      "Random Forest AUC: nan (nan)\n",
      "\n",
      "\n",
      "World Cup Dataset Results:\n",
      "Logistic Regression Accuracy: 0.059 (0.235)\n",
      "Logistic Regression f1 Score: 0.059 (0.235)\n",
      "Logistic Regression AUC: nan (nan)\n",
      "Support Vector Machine Accuracy: 0.118 (0.322)\n",
      "Support Vector Machine f1 Score: 0.118 (0.322)\n",
      "Support Vector Machine AUC: nan (nan)\n",
      "Random Forest Accuracy: 0.294 (0.456)\n",
      "Random Forest f1 Score: 0.294 (0.456)\n",
      "Random Forest AUC: nan (nan)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "cv = LeaveOneOut()\n",
    "lr = LogisticRegression()\n",
    "svm=SVC()\n",
    "clf = RandomForestClassifier(n_estimators = 100)\n",
    "datasets=[train_frenchNew,train_moaiNew,train_worldcupNew]\n",
    "idx=0\n",
    "for dataset in datasets:\n",
    "    if idx==0:\n",
    "        print(\"French Dataset Results:\")\n",
    "    if idx==1:\n",
    "        print(\"Moai Dataset Results:\")\n",
    "    if idx==2:\n",
    "        print(\"World Cup Dataset Results:\")\n",
    "    X=dataset.loc[:, dataset.columns == 'X']\n",
    "    y=dataset.loc[:, dataset.columns != 'X']\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    X_transformed = label_encoder.fit_transform(X)\n",
    "    X=X_transformed.reshape(-1,1)\n",
    "\n",
    "    scores_lracc = cross_val_score(lr, X, y, scoring='accuracy',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    scores_svmacc = cross_val_score(svm, X, y, scoring='accuracy',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    scores_clfacc=cross_val_score(clf, X, y, scoring='accuracy',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    \n",
    "    scores_lrf1 = cross_val_score(lr, X, y, scoring='f1_weighted',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    scores_svmf1 = cross_val_score(svm, X, y, scoring='f1_weighted',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    scores_clff1=cross_val_score(clf, X, y, scoring='f1_weighted',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    \n",
    "    scores_lrpre= cross_val_score(lr, X, y, scoring='precision',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    scores_svmpre = cross_val_score(svm, X, y, scoring='precision',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    scores_clfpre=cross_val_score(clf, X, y, scoring='precision',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    print('Logistic Regression Accuracy: %.3f (%.3f)' % (mean(scores_lracc), std(scores_lracc)))\n",
    "    print('Logistic Regression f1 Score: %.3f (%.3f)' % (mean(scores_lrf1), std(scores_lrf1)))\n",
    "    print('Logistic Regression AUC: %.3f (%.3f)' % (mean(scores_lrpre), std(scores_lrpre)))\n",
    "    \n",
    "    print('Support Vector Machine Accuracy: %.3f (%.3f)' % (mean(scores_svmacc), std(scores_svmacc)))\n",
    "    print('Support Vector Machine f1 Score: %.3f (%.3f)' % (mean(scores_svmf1), std(scores_svmf1)))\n",
    "    print('Support Vector Machine AUC: %.3f (%.3f)' % (mean(scores_svmpre), std(scores_svmpre)))\n",
    "    \n",
    "    print('Random Forest Accuracy: %.3f (%.3f)' % (mean(scores_clfacc), std(scores_clfacc)))\n",
    "    print('Random Forest f1 Score: %.3f (%.3f)' % (mean(scores_clff1), std(scores_clff1)))\n",
    "    print('Random Forest AUC: %.3f (%.3f)' % (mean(scores_clfpre), std(scores_clfpre)))\n",
    "    idx+=1\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ain311",
   "language": "python",
   "name": "ain311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
